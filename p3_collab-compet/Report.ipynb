{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 - Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will describe my solution to the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "\n",
    "## Learning Algorithm.\n",
    "\n",
    "The algorithm I selected is **Deep Deterministic Policy Gradient (DDPG) with Prioritized Experience Replay (PER)**. This is an extension of the solution I provide for P2 (Continuous control).\n",
    "DQN is described in the paper [\"Continuous Control With Deep Reinforcement Learning\"](https://arxiv.org/pdf/1509.02971.pdf), while PER is introduced here [\"Prioritized Experience Replay\"](https://arxiv.org/pdf/1511.05952)\n",
    "\n",
    "As an Actor-Critic method, DDPG uses two networks. \n",
    "* Actor - selects an action to take given a state\n",
    "* Critic - it \"scores\" the action selected by the actor, predicting \"how good\" the action is\n",
    "\n",
    "To separate calculations and work with more stationary targets, DQN used a second, separate network (**target network**) that lags behind the network we are using to train (**local network**). The weights of the local network are copied onto the target network, but this process only happens every few steps (**hard update**); we could also interpolate the values to try and get closer to the online ones (**soft update**), which is the option we have selected for this project. Either option will effectively \"lock\" our targets in place during that time.\n",
    "\n",
    "In DDPG, because we have two distinct networks, we need to keep our \"target\" and \"local\" versions of both Actor and Critic to add stability to the training.\n",
    "\n",
    "To allow some exploration in the Actor, we use an **Ornstein-Uhlenbeck process** for generating noise that will be added to our selected actions. It samples noise from a correlated normal distribution.\n",
    "\n",
    "The algorithm works as follows:\n",
    "![title](img/ddpg.png)\n",
    "\n",
    "### Prioritized Experience Replay\n",
    "\n",
    "DDPG utilizes a replay buffer to help comply with the [IID assumption](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). By storing our observations in a buffer, and uniformly sampling batches from that buffer, we get experiences that are less correlated. However, this method does not take into account the significance of each of the elements in the buffer. \n",
    "\n",
    "With PER, we build our mini-batches considering the value of the experiences in the buffer. Not all of those experiences are equally useful when the agents is trying to train from them, so we select those that maximize this learning value. This is measured by the magnitude of their TD error.\n",
    "\n",
    "In [Improving DDPG via Prioritized Experience Replay](https://cardwing.github.io/files/RL_course_report.pdf) DDPG was successfully improved by PER. The algorithm (DDPG with rank-based prioritization) looks as follows:\n",
    "\n",
    "![title](img/per.png)\n",
    "\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "To improve readability, the code has been split into different files:\n",
    "\n",
    "* agent_ddpg.py\n",
    "* model_ddpg.py\n",
    "* per_replay_buffer.py\n",
    "* ounoise.py\n",
    "* utils.py\n",
    "\n",
    "These files are commented to help make that code self-explanatory.\n",
    "\n",
    "In agent_ddpg.py - AgentOverlord::step, we add all of the experiences for all of the agents to the Replay Buffer before we try to learn from it. \n",
    "\n",
    "per_replay_buffer.py - in this case, we have adapted the implementation by Jonathan Pearce (https://github.com/Jonathan-Pearce/DDPG_PER/blob/master/PER_buffer.py) so it can work with the rest of the framework we had already constructed in \"p2 - continuous control\". This implementation makes use of a *Segment Tree*, defined in utils.py\n",
    "\n",
    "\n",
    "## Architecture and hyperparameters\n",
    "\n",
    "- Our Actor:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- **Three** *hidden layers* with **128 nodes each**.\n",
    "    -- An *output layer* with **4 nodes**.\n",
    "    - Applies **batch normalization** on each layer\n",
    "    - Uses **ReLU** as the activation function of the first two layers, and **Tanh** for the third layer. The latter was selected because the actions are composed of four continuous values in the range [-1..1]\n",
    "    \n",
    "    \n",
    "- Our Critic:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- A hidden layer with **256 nodes** + **4 extra inputs** to incorporate the action selected by the actor. \n",
    "    -- A second hidden layer with **256 nodes**\n",
    "    -- A third hidden layer with **128 nodes**\n",
    "    -- An *output layer* with **1 node**.\n",
    "    - Only the input layer is batch normalized.\n",
    "    - We use **ReLU** as the activation function.\n",
    "\n",
    "\n",
    "- We use an **Adam optimizer**\n",
    "\n",
    "\n",
    "- We use the following hyperparameters:\n",
    "    - *Gamma* (discount factor): **0.99**\n",
    "    - *Tau* (we use a soft update to update the weights of the target network): **1e-3**\n",
    "    - *Actor Learning Rate*: **1e-3**\n",
    "    - *Critic Learning Rate*: **1e-3**\n",
    "    - *Batch size*: **128**\n",
    "    - *Epsilon*: **0.01**\n",
    "    - *Alpha exponent* (to what extent the prioritization is used): **0.6** \n",
    "    - *Beta exponent* (to control how much importance-sampling weights correct the sample probability): **0.2**\n",
    "    - *Beta increment per sampling* (to modify beta overtime): **1e-3**\n",
    "\n",
    "\n",
    "- Lastly, our *replay buffer* size is **1e5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First, let's initialize the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from agent_ddpg import AgentOverlord\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the environment - please note we're using v2 of the problem (multiple agents)\n",
    "env = UnityEnvironment( file_name='Tennis_Windows_x86_64/Tennis.exe' )\n",
    "\n",
    "# Get brain - this will be used to control the unity environment\n",
    "brain_name = env.brain_names[ 0 ]\n",
    "brain = env.brains[ brain_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Agent initialization\n",
    "\n",
    "From the description of the problem, we know that:\n",
    "\n",
    "> In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "With this information, let's initialize our agents and get prepared to solve the problem (in this implementation, we are actually adding an \"agent overlord\" that will keep track of all of our agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 2\n",
      "Number of agents: 2\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# number of agents\n",
    "num_agents = states.shape[ 0 ]\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# hyperparameters\n",
    "actor_hidden = [128, 128, 128]\n",
    "actor_activation = [F.relu, F.relu, F.relu, T.tanh]\n",
    "\n",
    "critic_hidden = [256, 256, 128]\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "actor_learning_rate = 1e-3\n",
    "critic_learning_rate = 1e-3\n",
    "buffer_size = int(1e5)\n",
    "batch_size= 128\n",
    "seed = 19\n",
    "epsilon = 0.01\n",
    "alpha = 0.6\n",
    "beta = 0.2\n",
    "beta_increment_per_sampling = 1e-3\n",
    "\n",
    "# Initialize device\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize overlord\n",
    "agent_overlord = AgentOverlord( device, \n",
    "                                state_size, action_size, num_agents, \n",
    "                                actor_hidden, actor_activation,\n",
    "                                critic_hidden, \n",
    "                                gamma, tau, actor_learning_rate, critic_learning_rate, \n",
    "                                buffer_size, batch_size, \n",
    "                                seed, \n",
    "                                epsilon, alpha, beta, beta_increment_per_sampling )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem\n",
    "\n",
    "We are now ready to start solving the problem.\n",
    "\n",
    "From the problem description:\n",
    "\n",
    "> The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, after each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 score, which yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.\n",
    "\n",
    "We are setting a maximum number of episodes of *5000*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00470\n",
      "Episode 200\tAverage Score: 0.02060\n",
      "Episode 300\tAverage Score: 0.05330\n",
      "Episode 400\tAverage Score: 0.06990\n",
      "Episode 500\tAverage Score: 0.08580\n",
      "Episode 600\tAverage Score: 0.09010\n",
      "Episode 700\tAverage Score: 0.09320\n",
      "Episode 800\tAverage Score: 0.25300\n",
      "Episode 900\tAverage Score: 0.15880\n",
      "Episode 1000\tAverage Score: 0.31580\n",
      "Episode 1026\tAverage Score: 0.50960Sucess in 1026 episodes!\n",
      "Mean score was: 0.50960, above the criterium 0.50000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_EPISODES = 5000\n",
    "MIN_SUCCESS_MEAN_SCORE = 0.5\n",
    "\n",
    "EPISODE_RANGE_COUNT = 100\n",
    "\n",
    "scores_deque = deque(maxlen=EPISODE_RANGE_COUNT)\n",
    "scores = []\n",
    "\n",
    "success = False\n",
    "for episode in range(1,MAX_EPISODES):\n",
    "    \n",
    "    # let's restart the environment and get the initial states of the agents\n",
    "    env_info = env.reset( train_mode=True )[ brain_name ]    \n",
    "    states = env_info.vector_observations\n",
    "    score = np.zeros(num_agents)\n",
    "    agent_overlord.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # choose the actions we should take\n",
    "        actions = agent_overlord.act( states )\n",
    "\n",
    "        # apply them\n",
    "        env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # update the overlord\n",
    "        agent_overlord.step( states, actions, rewards, next_states, dones )\n",
    "\n",
    "        # update states for next iteration\n",
    "        states = next_states\n",
    "\n",
    "        score += env_info.rewards\n",
    "\n",
    "        if np.any( dones ):\n",
    "            # episode is done\n",
    "            break\n",
    "\n",
    "    # Gets max score obtained by either agent\n",
    "    max_score = np.max(score)\n",
    "\n",
    "    scores_deque.append(max_score)           \n",
    "    scores.append(max_score)\n",
    "    \n",
    "    # Calculate the mean score so far \n",
    "    mean_score_so_far = np.mean(scores_deque)\n",
    "        \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.5f}'.format(episode, mean_score_so_far), end=\"\")\n",
    "    if episode % EPISODE_RANGE_COUNT == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.5f}'.format(episode, mean_score_so_far))\n",
    "\n",
    "    # If our deque is full, this means we have enough episodes to potentially end the experiment.\n",
    "    # In that case, if the mean of the scores in the last episodes is over the success criterium\n",
    "    if len(scores_deque) == EPISODE_RANGE_COUNT and mean_score_so_far > MIN_SUCCESS_MEAN_SCORE:\n",
    "        # End experiment\n",
    "        success = True\n",
    "        break\n",
    "    \n",
    "if success:\n",
    "    print(\"Sucess in {:d} episodes!\".format(episode))\n",
    "    print( \"Mean score was: {:.5f}, above the criterium {:.5f}.\".format(mean_score_so_far, MIN_SUCCESS_MEAN_SCORE))\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent_overlord.save()\n",
    "else:\n",
    "    print(\"We did not reach the desired score {:.5f}/{:.5f}\".format(mean_score_so_far, MIN_SUCCESS_MEAN_SCORE))\n",
    "\n",
    "# Plot a figure showing the mean score per episode\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test\n",
    "\n",
    "If our experiment succeeded in the previous step, that means we now have a trained model. Let's  the environment once more, and look at how the agents perform in real-time for one episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, load the saved model\n",
    "# - Actually, we don't need to do this, as the overlord is still loaded at this point\n",
    "# - But this is what we would have to do if we need to load it from file\n",
    "# agent_overlord.reset()\n",
    "# agent_overlord.load()\n",
    "\n",
    "env_info = env.reset( train_mode=False )[ brain_name ]    \n",
    "states = env_info.vector_observations\n",
    "    \n",
    "episode_num = 5\n",
    "for i in range(episode_num):\n",
    "    while True:\n",
    "        # choose the actions we should take\n",
    "        actions = agent_overlord.act( states )\n",
    "\n",
    "        # apply them\n",
    "        env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "        dones = env_info.local_done\n",
    "        if np.any( dones ):\n",
    "            # episode is done\n",
    "            break\n",
    "\n",
    "        # update states for next iteration\n",
    "        states = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, finally, close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a video recording of the test results running in Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/cJtI-rwTXxI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2098d3dc0b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://www.youtube.com/embed/cJtI-rwTXxI', width='560', height='315')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "The first thing we could try to improve our solution is to study the effects of the different hyperparameters on the results we obtain. Could we use more hidden layers? Larger or shorter ones? Are our tau, gamma, etc. correct, or can we choose something better? \n",
    "\n",
    "I work in AAA videogames. The solution chosen uses a Replay Buffer, which can potentially consume a lot of memory (depending on the problem). Multithreading a solution can sometimes be easier than optimizing its memory usage. This makes algorithms such as **Advantage Actor-Critic (A3C)** interesting, as they do not rely on \"memory\" to fight correlation, but instead use the observations of multiple independent agents.\n",
    "\n",
    "I would also like to try [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275), as it looks like it might outperform my solution.\n",
    "\n",
    "It would be interesting to compare the performance and results of these two different approaches, as well of their memory requirements, to get more data that can inform whether this type of RL is feasible for a modern AAA videogame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
