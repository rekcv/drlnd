{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 - Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will describe my solution to the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "\n",
    "## Learning Algorithm.\n",
    "\n",
    "The algorithm I selected is **Deep Deterministic Policy Gradient (DDPG) with Prioritized Experience Replay (PER)**. This is an extension of the solution I provide for P2 (Continuous control).\n",
    "DQN is described in the paper [\"Continuous Control With Deep Reinforcement Learning\"](https://arxiv.org/pdf/1509.02971.pdf), while PER is introduced here [\"Prioritized Experience Replay\"](https://arxiv.org/pdf/1511.05952)\n",
    "\n",
    "As an Actor-Critic method, DDPG uses two networks. \n",
    "* Actor - selects an action to take given a state\n",
    "* Critic - it \"scores\" the action selected by the actor, predicting \"how good\" the action is\n",
    "\n",
    "To separate calculations and work with more stationary targets, DQN used a second, separate network (**target network**) that lags behind the network we are using to train (**local network**). The weights of the local network are copied onto the target network, but this process only happens every few steps (**hard update**); we could also interpolate the values to try and get closer to the online ones (**soft update**), which is the option we have selected for this project. Either option will effectively \"lock\" our targets in place during that time.\n",
    "\n",
    "In DDPG, because we have two distinct networks, we need to keep our \"target\" and \"local\" versions of both Actor and Critic to add stability to the training.\n",
    "\n",
    "To allow some exploration in the Actor, we use an **Ornstein-Uhlenbeck process** for generating noise that will be added to our selected actions. It samples noise from a correlated normal distribution.\n",
    "\n",
    "The algorithm works as follows:\n",
    "![title](img/ddpg.png)\n",
    "\n",
    "### Prioritized Experience Replay\n",
    "\n",
    "DDPG utilizes a replay buffer to help comply with the [IID assumption](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). By storing our observations in a buffer, and uniformly sampling batches from that buffer, we get experiences that are less correlated. However, this method does not take into account the significance of each of the elements in the buffer. \n",
    "\n",
    "With PER, we build our mini-batches considering the value of the experiences in the buffer. Not all of those experiences are equally useful when the agents is trying to train from them, so we select those that maximize this learning value. This is measured by the magnitude of their TD error.\n",
    "\n",
    "The algorithm (applied to a Double DQN problem) works as follows:\n",
    "![title](img/per.png)\n",
    "\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "To improve readability, the code has been split into different files:\n",
    "\n",
    "* agent_ddpg.py\n",
    "* model_ddpg.py\n",
    "* replay_buffer.py\n",
    "* ounoise.py\n",
    "\n",
    "These files are commented to help make that code self-explanatory.\n",
    "\n",
    "However, it is important to note here that since we are solving the multiagent version of the problem, the algorithm differs slightly from a traditional DDPG implementation. In our case, the environment will give us information for every agent at once, which means we do not deal with a single \"state, action, reward, next_state, done\" tuple, but with a list of them. \n",
    "\n",
    "In agent_ddpg.py - AgentOverlord::step, we add all of the experiences for all of the agents to the Replay Buffer before we try to learn from it. The rest of the algorithm is similar, but in our case we are generating multiple actions at once (one per agent) based on the lists of observations we receive from the environment.\n",
    "\n",
    "## Architecture and hyperparameters\n",
    "\n",
    "- Our Actor:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- **Two** *hidden layers* with **128 nodes each**.\n",
    "    -- An *output layer* with **4 nodes**.\n",
    "    - Applies **batch normalization** on each layer\n",
    "    - Uses **ReLU** as the activation function of the first two layers, and **Tanh** for the third layer. The latter was selected because the actions are composed of four continuous values in the range [-1..1]\n",
    "    \n",
    "    \n",
    "- Our Critic:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- A hidden layer with **256 nodes** + **4 extra inputs** to incorporate the action selected by the actor. \n",
    "    -- An extra hidden layer with **256 nodes**\n",
    "    -- An *output layer* with **1 node**.\n",
    "    - Only the input layer is batch normalized.\n",
    "    - We use **ReLU** as the activation function.\n",
    "\n",
    "- We use an **Adam optimizer**\n",
    "\n",
    "- We use the following hyperparameters:\n",
    "    - *Gamma* (discount factor): **0.99**\n",
    "    - *Tau* (we use a soft update to update the weights of the target network): **1e-3**\n",
    "    - *Actor Learning Rate*: **1e-3**\n",
    "    - *Critic Learning Rate*: **1e-3**\n",
    "    - *Batch size*: **128**\n",
    "    - *Epsilon*: **0.01**\n",
    "    - *Alpha exponent*: **0.6**\n",
    "    - *Beta exponent*: **0.4**\n",
    "    - *Beta increment per sampling*: **1e-3**\n",
    "    \n",
    "- Lastly, our *replay buffer* size is **1e5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First, let's initialize the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from agent_ddpg import AgentOverlord\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the environment - please note we're using v2 of the problem (multiple agents)\n",
    "env = UnityEnvironment( file_name='Tennis_Windows_x86_64/Tennis.exe' )\n",
    "\n",
    "# Get brain - this will be used to control the unity environment\n",
    "brain_name = env.brain_names[ 0 ]\n",
    "brain = env.brains[ brain_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Agent initialization\n",
    "\n",
    "From the description of the problem, we know that:\n",
    "\n",
    "> \"In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. \n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\"\n",
    "\n",
    "With this information, let's initialize our agents and get prepared to solve the problem (in this implementation, we are actually adding an \"agent overlord\" that will keep track of all of our agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 2\n",
      "Number of agents: 2\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# number of agents\n",
    "num_agents = states.shape[ 0 ]\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# hyperparameters\n",
    "actor_hidden = [128, 128]\n",
    "actor_activation = [F.relu, F.relu, T.tanh]\n",
    "\n",
    "critic_hidden = [256, 256]\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "actor_learning_rate = 1e-3\n",
    "critic_learning_rate = 1e-3\n",
    "buffer_size = int(1e5)\n",
    "batch_size= 128\n",
    "seed = 19\n",
    "epsilon = 0.01\n",
    "alpha = 0.6\n",
    "beta = 0.4\n",
    "beta_increment_per_sampling = 1e-3\n",
    "\n",
    "# Initialize device\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize overlord\n",
    "agent_overlord = AgentOverlord( device, \n",
    "                                state_size, action_size, num_agents, \n",
    "                                actor_hidden, actor_activation,\n",
    "                                critic_hidden, \n",
    "                                gamma, tau, actor_learning_rate, critic_learning_rate, \n",
    "                                buffer_size, batch_size, \n",
    "                                seed, \n",
    "                                epsilon, alpha, beta, beta_increment_per_sampling )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem\n",
    "\n",
    "We are now ready to start solving the problem.\n",
    "\n",
    "We are setting a maximum number of episodes of *1500*. The minimum mean score over the last *100* episodes needs to be over *30*, as described in the problem, to consider the experiment a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0d405d9f2a5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# update the overlord\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0magent_overlord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# update states for next iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-reinforcement-learning\\drlnd\\p3_collab-compet\\agent_ddpg.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# We're receiving multiple samples (one per agent). Let's add them all to our replay buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# If we have enough samples in our Replay Buffer...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-reinforcement-learning\\drlnd\\p3_collab-compet\\agent_ddpg.py\u001b[0m in \u001b[0;36mappend_sample\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mappend_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mold_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mtarget_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'action'"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MIN_SUCCESS_MEAN_SCORE = 0.5\n",
    "\n",
    "EPISODE_RANGE_COUNT = 100\n",
    "\n",
    "scores_deque = deque(maxlen=EPISODE_RANGE_COUNT)\n",
    "scores = []\n",
    "\n",
    "success = False\n",
    "for episode in range(1,MAX_EPISODES):\n",
    "    \n",
    "    # let's restart the environment and get the initial states of the agents\n",
    "    env_info = env.reset( train_mode=True )[ brain_name ]    \n",
    "    states = env_info.vector_observations\n",
    "    score = np.zeros(num_agents)\n",
    "    agent_overlord.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # choose the actions we should take\n",
    "        actions = agent_overlord.act( states )\n",
    "\n",
    "        # apply them\n",
    "        env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # update the overlord\n",
    "        agent_overlord.step( states, actions, rewards, next_states, dones )\n",
    "\n",
    "        # update states for next iteration\n",
    "        states = next_states\n",
    "\n",
    "        score += env_info.rewards\n",
    "\n",
    "        if np.any( dones ):\n",
    "            # episode is done\n",
    "            break\n",
    "\n",
    "    # Mean score over all agents\n",
    "    mean_score = np.mean(score)\n",
    "\n",
    "    scores_deque.append(mean_score)           \n",
    "    scores.append(mean_score)\n",
    "    \n",
    "    # Calculate the mean score so far \n",
    "    mean_score_so_far = np.mean(scores_deque)\n",
    "        \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.5f}'.format(episode, mean_score_so_far), end=\"\")\n",
    "    if episode % EPISODE_RANGE_COUNT == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.5f}'.format(episode, mean_score_so_far))\n",
    "\n",
    "    # If our deque is full, this means we have enough episodes to potentially end the experiment.\n",
    "    # In that case, if the mean of the scores in the last episodes is over the success criterium\n",
    "    if len(scores_deque) == EPISODE_RANGE_COUNT and mean_score_so_far > MIN_SUCCESS_MEAN_SCORE:\n",
    "        # End experiment\n",
    "        success = True\n",
    "        break\n",
    "    \n",
    "if success:\n",
    "    print(\"Sucess in {:d} episodes!\".format(episode))\n",
    "    print( \"Mean score was: {:.5f}, above the criterium {:.5f}.\".format(mean_score_so_far, MIN_SUCCESS_MEAN_SCORE))\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent_overlord.save()\n",
    "else:\n",
    "    print(\"We did not reach the desired score {:.5f}/{:.5f}\".format(mean_score_so_far, MIN_SUCCESS_MEAN_SCORE))\n",
    "\n",
    "# Plot a figure showing the mean score per episode\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test\n",
    "\n",
    "If our experiment succeeded in the previous step, that means we now have a trained model. Let's  the environment once more, and look at how the agents perform in real-time for one episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, load the saved model\n",
    "# - Actually, we don't need to do this, as the overlord is still loaded at this point\n",
    "# - But this is what we would have to do if we need to load it from file\n",
    "# agent_overlord.reset()\n",
    "# agent_overlord.load()\n",
    "\n",
    "env_info = env.reset( train_mode=False )[ brain_name ]    \n",
    "states = env_info.vector_observations\n",
    "    \n",
    "while True:\n",
    "    # choose the actions we should take\n",
    "    actions = agent_overlord.act( states )\n",
    "\n",
    "    # apply them\n",
    "    env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "    dones = env_info.local_done\n",
    "    if np.any( dones ):\n",
    "        # episode is done\n",
    "        break\n",
    "    \n",
    "    # update states for next iteration\n",
    "    states = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, finally, close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a video recording of the test results running in Unity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://www.youtube.com/embed/O17ooxWvI3Q', width='560', height='315')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "The first thing we could try to improve our solution is to study the effects of the different hyperparameters on the results we obtain. Could we use more hidden layers? Larger or shorter ones? Are our tau, gamma, etc. correct, or can we choose something better?\n",
    "\n",
    "Secondly, I mentioned I worked in AAA videogames. The solution chosen uses a Replay Buffer, which can potentially consume a lot of memory (depending on the problem). Multithreading a solution can sometimes be easier than optimizing its memory usage. This makes algorithms such as **Advantage Actor-Critic (A3C)** interesting, as they do not rely on \"memory\" to fight correlation, but instead use the observations of multiple independent agents.\n",
    "\n",
    "It would be interesting to compare the performance and results of these two different approaches, as well of their memory requirements, to get more data that can inform whether this type of RL is feasible for a modern AAA videogame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
