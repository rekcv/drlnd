{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2 - Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will describe my solution to the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "## Learning Algorithm.\n",
    "\n",
    "I have chosen the multi-agent version of the problem, as it sounded like a better approach to being able to apply any solution like this to my field of work (AAA videogame industry).\n",
    "\n",
    "The algorithm I selected is **Deep Deterministic Policy Gradient (DDPG)**. This is a good continuation to my solution for (P1 - Navigation), as I had then implemented Deep Q-Network (DQN) to solve that problem. The algorithm is described in the paper [\"Continuous Control With Deep Reinforcement Learning\"](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "\n",
    "As in DQN, DDPG utilizes a replay buffer to help comply with the [IID assumption](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). By storing our observations in a buffer, and sampling batches from that buffer, we get experiences that are less correlated. In this implementation, I have re-used the ReplayBuffer that I implemented for P1.\n",
    "\n",
    "DDPG combines the ideas from DQN and Deterministic Policy Gradient (DPG). The latter allows us to solve continuous action space problems.\n",
    "\n",
    "Like an Actor-Critic method, DDPG uses two networks. \n",
    "* Actor - selects an action to take given a state\n",
    "* Critic - it \"scores\" the action selected by the actor, predicting \"how good\" the action is\n",
    "\n",
    "To separate calculations and work with more stationary targets, DQN used a second, separate network (**target network**) that lags behind the network we are using to train (**local network**). The weights of the local network are copied onto the target network, but this process only happens every few steps (**hard update**); we could also interpolate the values to try and get closer to the online ones (**soft update**), which is the option we have selected for this project. Either option will effectively \"lock\" our targets in place during that time.\n",
    "\n",
    "In DDPG, because we have two distinct networks, we need to keep our \"target\" and \"local\" versions of both Actor and Critic to add stability to the training.\n",
    "\n",
    "To allow some exploration in the Actor, we use an **Ornstein-Uhlenbeck process** for generating noise that will be added to our selected actions. It samples noise from a correlated normal distribution.\n",
    "\n",
    "The algorithm works as follows:\n",
    "![title](img/ddpg.png)\n",
    "\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "To improve readability, the code has been split into different files:\n",
    "\n",
    "* agent_ddpg.py\n",
    "* model_ddpg.py\n",
    "* replay_buffer.py\n",
    "* ounoise.py\n",
    "\n",
    "These files are commented to help make that code self-explanatory.\n",
    "\n",
    "However, it is important to note here that since we are solving the multiagent version of the problem, the algorithm differs slightly from a traditional DDPG implementation. In our case, the environment will give us information for every agent at once, which means we do not deal with a single \"state, action, reward, next_state, done\" tuple, but with a list of them. \n",
    "\n",
    "In agent_ddpg.py - AgentOverlord::step, we add all of the experiences for all of the agents to the Replay Buffer before we try to learn from it. The rest of the algorithm is similar, but in our case we are generating multiple actions at once (one per agent) based on the lists of observations we receive from the environment.\n",
    "\n",
    "## Architecture and hyperparameters\n",
    "\n",
    "- Our Actor:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- **Two** *hidden layers* with **128 nodes each**.\n",
    "    -- An *output layer* with **4 nodes**.\n",
    "    - Applies **batch normalization** on each layer\n",
    "    - Uses **ReLU** as the activation function of the first two layers, and **Tanh** for the third layer. The latter was selected because the actions are composed of four continuous values in the range [-1..1]\n",
    "    \n",
    "    \n",
    "- Our Critic:\n",
    "    - Uses a deep neural network with:\n",
    "    -- An *input layer* with **33 nodes**.\n",
    "    -- A hidden layer with **256 nodes** + **4 extra inputs** to incorporate the action selected by the actor. \n",
    "    -- An extra hidden layer with **256 nodes**\n",
    "    -- An *output layer* with **1 node**.\n",
    "    - Only the input layer is batch normalized.\n",
    "    - We use **ReLU** as the activation function.\n",
    "\n",
    "- We use an **Adam optimizer**\n",
    "\n",
    "- We use the following hyperparameters:\n",
    "    - *Gamma* (discount factor): **0.99**\n",
    "    - *Tau* (we use a soft update to update the weights of the target network): **1e-3**\n",
    "    - *Actor Learning Rate*: **1e-3**\n",
    "    - *Critic Learning Rate*: **1e-3**\n",
    "    - *Batch size*: **128**\n",
    "    - *Update_every* (how many steps need to run before selecting a new batch): **4**\n",
    "    \n",
    "- Lastly, our *replay buffer* size is **1e5**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First, let's initialize the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from agent_ddpg import AgentOverlord\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the environment - please note we're using v2 of the problem (multiple agents)\n",
    "env = UnityEnvironment( file_name='Reacher_Windows_x86_64/Reacher.exe' )\n",
    "\n",
    "# Get brain - this will be used to control the unity environment\n",
    "brain_name = env.brain_names[ 0 ]\n",
    "brain = env.brains[ brain_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Agent initialization\n",
    "\n",
    "From the description of the problem, we know that:\n",
    "\n",
    "> \"In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. \n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\"\n",
    "\n",
    "With this information, let's initialize our agents and get prepared to solve the problem (in this implementation, we are actually adding an \"agent overlord\" that will keep track of all of our agents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "Number of agents: 20\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# number of agents\n",
    "num_agents = states.shape[ 0 ]\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# hyperparameters\n",
    "actor_hidden = [128, 128]\n",
    "actor_activation = [F.relu, F.relu, T.tanh]\n",
    "\n",
    "critic_hidden = [256, 256]\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "actor_learning_rate = 1e-3\n",
    "critic_learning_rate = 1e-3\n",
    "buffer_size = int(1e5)\n",
    "batch_size= 128\n",
    "seed = 12\n",
    "\n",
    "# Initialize device\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize overlord\n",
    "agent_overlord = AgentOverlord( device, \n",
    "                                state_size, action_size, num_agents, \n",
    "                                actor_hidden, actor_activation,\n",
    "                                critic_hidden, \n",
    "                                gamma, tau, actor_learning_rate, critic_learning_rate, \n",
    "                                buffer_size, batch_size, \n",
    "                                seed )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the problem\n",
    "\n",
    "We are now ready to start solving the problem.\n",
    "\n",
    "We are setting a maximum number of episodes of *1500*. The minimum mean score over the last *100* episodes needs to be over *30*, as described in the problem, to consider the experiment a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 1500\n",
    "MIN_SUCCESS_MEAN_SCORE = 30\n",
    "\n",
    "EPISODE_RANGE_COUNT = 100\n",
    "\n",
    "scores_deque = deque(maxlen=EPISODE_RANGE_COUNT)\n",
    "scores = []\n",
    "last_mean_score = 0.5\n",
    "\n",
    "success = False\n",
    "for episode in range(1,MAX_EPISODES):\n",
    "    \n",
    "    # let's restart the environment and get the initial states of the agents\n",
    "    env_info = env.reset( train_mode=True )[ brain_name ]    \n",
    "    states = env_info.vector_observations\n",
    "    score = np.zeros(num_agents)\n",
    "    agent_overlord.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # choose the actions we should take\n",
    "        actions = agent_overlord.act( states )\n",
    "\n",
    "        # apply them\n",
    "        env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # update the overlord\n",
    "        agent_overlord.step( states, actions, rewards, next_states, dones )\n",
    "\n",
    "        # update states for next iteration\n",
    "        states = next_states\n",
    "\n",
    "        score += env_info.rewards\n",
    "\n",
    "        if np.any( dones ):\n",
    "            # episode is done\n",
    "            break\n",
    "\n",
    "    # Mean score over all agents\n",
    "    mean_score = np.mean(score)\n",
    "\n",
    "    scores_deque.append(mean_score)           \n",
    "    scores.append(mean_score)\n",
    "    \n",
    "    # Calculate the mean score so far \n",
    "    mean_score_so_far = np.mean(scores_deque)\n",
    "        \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, mean_score_so_far), end=\"\")\n",
    "    if episode % EPISODE_RANGE_COUNT == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, mean_score_so_far))\n",
    "\n",
    "    # If our deque is full, this means we have enough episodes to potentially end the experiment.\n",
    "    # In that case, if the mean of the scores in the last episodes is over the success criterium\n",
    "    if len(scores_deque) == EPISODE_RANGE_COUNT and mean_score_so_far > MIN_SUCCESS_MEAN_SCORE:\n",
    "        # End experiment\n",
    "        success = True\n",
    "        break\n",
    "    \n",
    "if success:\n",
    "    print(\"Sucess in {:i} episodes!\", episode)\n",
    "    print( \"Mean score was: {:.2f}, above the criterium {:.2f}.\", mean_score_so_far, MIN_SUCCESS_MEAN_SCORE)\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent_overlord.save()\n",
    "else:\n",
    "    print(\"We did not reach the desired score {.2f/.2f}\", mean_score_so_far, MIN_SUCCESS_MEAN_SCORE)\n",
    "\n",
    "# Plot a figure showing the mean score per episode\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show\n",
    "\n",
    "# And close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test\n",
    "\n",
    "If our experiment succeeded in the previous step, that means we now have a trained model. Let's  the environment once more, and look at how the agents perform in real-time for one episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start another overlord\n",
    "agent_overlord = AgentOverlord( device, \n",
    "                                state_size, action_size, num_agents, \n",
    "                                actor_hidden, actor_activation,\n",
    "                                critic_hidden, \n",
    "                                gamma, tau, actor_learning_rate, critic_learning_rate, \n",
    "                                buffer_size, batch_size, \n",
    "                                seed )\n",
    "\n",
    "# Now, load the saved model\n",
    "agent_overlord.load()\n",
    "\n",
    "env_info = env.reset( train_mode=False )[ brain_name ]    \n",
    "states = env_info.vector_observations\n",
    "    \n",
    "while True:\n",
    "    # choose the actions we should take\n",
    "    actions = agent_overlord.act( states )\n",
    "\n",
    "    # apply them\n",
    "    env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "    dones = env_info.local_done\n",
    "    if np.any( dones ):\n",
    "        # episode is done\n",
    "        break\n",
    "    \n",
    "    # update states for next iteration\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "# And, finally, close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "The first thing we could try to improve our solution is to study the effects of the different hyperparameters on the results we obtain. Could we use more hidden layers? Larger or shorter ones? Are our tau, gamma, etc. correct, or can we choose something better?\n",
    "\n",
    "Secondly, I mentioned I worked in AAA videogames. The solution chosen uses a Replay Buffer, which can potentially consume a lot of memory (depending on the problem). Multithreading a solution can sometimes be easier than optimizing its memory usage. This makes algorithms such as **Advantage Actor-Critic (A3C)** interesting, as they do not rely on \"memory\" to fight correlation, but instead use the observations of multiple independent agents.\n",
    "\n",
    "It would be interesting to compare the performance and results of these two different approaches, as well of their memory requirements, to get more data that can inform whether this type of RL is feasible for a modern AAA videogame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
