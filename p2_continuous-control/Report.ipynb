{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2 - Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will describe my solution to the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Learning Algorithm.\n",
    "\n",
    "I have chosen the multi-agent version of the problem, as I wanted to apply A3C (Asynchronous Advantage Actor Critic) to it. Using this multi-agent approach, we also remove the need to use a replay memory, as the model is getting its uncorrelated data from the various agents that are trying to solve the problem at once. This sounded like a better approach to being able to apply any solution like this to my field of work (AAA videogame industry).\n",
    "\n",
    "\n",
    "### 2. Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from agent import AgentOverlord\n",
    "\n",
    "# Initialize the environment - please note we're using v2 of the problem (multiple agents)\n",
    "env = UnityEnvironment( file_name='Reacher_Windows_x86_64/Reacher.exe' )\n",
    "\n",
    "# Get brain - this will be used to control the unity environment\n",
    "brain_name = env.brain_names[ 0 ]\n",
    "brain = env.brains[ brain_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Agent initialization\n",
    "\n",
    "From the description of the problem, we know that:\n",
    "\n",
    "> \"In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. \n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\"\n",
    "\n",
    "With this information, let's initialize our agents and get prepared to solve the problem (in this implementation, we are actually adding an \"agent overlord\" that will keep track of all of our agents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "agent_num = states.shape[ 0 ]\n",
    "\n",
    "# hyperparameters\n",
    "actor_layer_size=128\n",
    "critic_layer_size = 128\n",
    "gamma = 0.99\n",
    "learning_rate=5e-4\n",
    "\n",
    "# initialize overlord\n",
    "agent_overlord = AgentOverlord( state_size, action_size, agent_num, actor_layer_size, critic_layer_size, gamma, learning_rate )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solve the problem\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0724, -0.0669,  0.4784, -0.5794]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0522]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7953,  0.7281,  1.5227, -0.5991]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.5573]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2042, -0.0577, -0.4668,  0.8420]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.2024]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1987, -0.0268,  0.2881,  0.6461]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6942]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.5091,  0.6406,  0.0729, -0.4004]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.5292]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4758, -0.8860, -0.4612,  0.5132]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.3844]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7467, -0.7518, -0.0390,  0.7521]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2164]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3472,  0.4399, -0.1160,  0.4019]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.6880]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0870, -0.0796, -0.2866,  0.1277]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.1586]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1814, -0.1919, -0.5260, -0.2859]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2914]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.9722, -1.1713, -0.6828, -0.0125]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0820]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4467, -0.2937,  0.3974,  0.8538]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.7705]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4636, -0.0286, -0.0959,  0.4498]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.4227]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0691,  0.1010, -0.2175, -1.5235]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0848]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2569,  0.4633,  0.3359,  0.2938]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7389]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1083, -0.0540,  0.7585,  0.1792]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.9627]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1409,  0.8271,  0.1066, -0.6454]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.2487]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5468e-01, -8.9849e-04, -1.9651e-01, -1.1136e+00]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[0.8637]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.3462,  0.0905,  0.2843, -0.3483]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.9975]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2276, -0.0396,  0.5414,  0.3857]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.7116]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "UnityActionException",
     "evalue": "There was a mismatch between the provided action and environment's expectation: The brain ReacherBrain expected 80 continuous action(s), but was provided: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityActionException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1c4c3597636b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# apply them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mbrain_name\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[0;32m    364\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_brains\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_action_space_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_brains\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_action_space_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m                         str(vector_action[b])))\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             outputs = self.communicator.exchange(\n",
      "\u001b[1;31mUnityActionException\u001b[0m: There was a mismatch between the provided action and environment's expectation: The brain ReacherBrain expected 80 continuous action(s), but was provided: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
     ]
    }
   ],
   "source": [
    "# let's restart the environment and get the initial states of the agents\n",
    "env_info = env.reset( train_mode=True )[ brain_name ]    \n",
    "states = env_info.vector_observations\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # choose the actions we should take\n",
    "    actions = agent_overlord.choose_actions( states )\n",
    "    \n",
    "    # apply them\n",
    "    env_info = env.step( actions )[ brain_name ]\n",
    "    \n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.dones\n",
    "    \n",
    "    # update the overlord\n",
    "    agent_overlord.step( states, actions, rewards, next_states, dones )\n",
    "    \n",
    "    # update states for next iteration\n",
    "    states = next_states\n",
    "    \n",
    "    if np.any( dones ):\n",
    "        print(\"there was a done\")\n",
    "        break\n",
    "\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
