{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2 - Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will describe my solution to the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Learning Algorithm.\n",
    "\n",
    "I have chosen the multi-agent version of the problem, as I wanted to apply A3C (Asynchronous Advantage Actor Critic) to it. Using this multi-agent approach, we also remove the need to use a replay memory, as the model is getting its uncorrelated data from the various agents that are trying to solve the problem at once. This sounded like a better approach to being able to apply any solution like this to my field of work (AAA videogame industry).\n",
    "\n",
    "\n",
    "### 2. Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from agent import AgentOverlord\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the environment - please note we're using v2 of the problem (multiple agents)\n",
    "env = UnityEnvironment( file_name='Reacher_Windows_x86_64/Reacher.exe' )\n",
    "\n",
    "# Get brain - this will be used to control the unity environment\n",
    "brain_name = env.brain_names[ 0 ]\n",
    "brain = env.brains[ brain_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Agent initialization\n",
    "\n",
    "From the description of the problem, we know that:\n",
    "\n",
    "> \"In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. \n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\"\n",
    "\n",
    "With this information, let's initialize our agents and get prepared to solve the problem (in this implementation, we are actually adding an \"agent overlord\" that will keep track of all of our agents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 4\n",
      "Number of agents: 20\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "# number of agents\n",
    "agent_num = states.shape[ 0 ]\n",
    "print('Number of agents:', agent_num)\n",
    "\n",
    "# hyperparameters\n",
    "actor_layer_size=128\n",
    "critic_layer_size = 128\n",
    "gamma = 0.99\n",
    "learning_rate=5e-4\n",
    "\n",
    "# initialize overlord\n",
    "agent_overlord = AgentOverlord( state_size, action_size, agent_num, actor_layer_size, critic_layer_size, gamma, learning_rate )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solve the problem\n",
    "\n",
    "We are now ready to start solving the problem. To do so, we will reset our environment and step forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Total score (averaged over agents): 0.12\n",
      "Episode 2 - Total score (averaged over agents): 0.08\n",
      "Episode 3 - Total score (averaged over agents): 0.05\n",
      "Episode 4 - Total score (averaged over agents): 0.18\n",
      "Episode 5 - Total score (averaged over agents): 0.19\n",
      "Episode 6 - Total score (averaged over agents): 0.17\n",
      "Episode 7 - Total score (averaged over agents): 0.09\n",
      "Episode 8 - Total score (averaged over agents): 0.07\n",
      "Episode 9 - Total score (averaged over agents): 0.18\n",
      "Episode 10 - Total score (averaged over agents): 0.09\n",
      "Episode 11 - Total score (averaged over agents): 0.15\n",
      "Episode 12 - Total score (averaged over agents): 0.14\n",
      "Episode 13 - Total score (averaged over agents): 0.13\n",
      "Episode 14 - Total score (averaged over agents): 0.07\n",
      "Episode 15 - Total score (averaged over agents): 0.16\n",
      "Episode 16 - Total score (averaged over agents): 0.12\n",
      "Episode 17 - Total score (averaged over agents): 0.18\n",
      "Episode 18 - Total score (averaged over agents): 0.15\n",
      "Episode 19 - Total score (averaged over agents): 0.06\n",
      "Episode 20 - Total score (averaged over agents): 0.16\n",
      "Episode 21 - Total score (averaged over agents): 0.07\n",
      "Episode 22 - Total score (averaged over agents): 0.13\n",
      "Episode 23 - Total score (averaged over agents): 0.16\n",
      "Episode 24 - Total score (averaged over agents): 0.11\n",
      "Episode 25 - Total score (averaged over agents): 0.09\n",
      "Episode 26 - Total score (averaged over agents): 0.12\n",
      "Episode 27 - Total score (averaged over agents): 0.18\n",
      "Episode 28 - Total score (averaged over agents): 0.18\n",
      "Episode 29 - Total score (averaged over agents): 0.21\n",
      "Episode 30 - Total score (averaged over agents): 0.18\n",
      "Episode 31 - Total score (averaged over agents): 0.05\n",
      "Episode 32 - Total score (averaged over agents): 0.10\n",
      "Episode 33 - Total score (averaged over agents): 0.13\n",
      "Episode 34 - Total score (averaged over agents): 0.07\n",
      "Episode 35 - Total score (averaged over agents): 0.23\n",
      "Episode 36 - Total score (averaged over agents): 0.10\n",
      "Episode 37 - Total score (averaged over agents): 0.17\n",
      "Episode 38 - Total score (averaged over agents): 0.13\n",
      "Episode 39 - Total score (averaged over agents): 0.13\n",
      "Episode 40 - Total score (averaged over agents): 0.12\n",
      "Episode 41 - Total score (averaged over agents): 0.10\n",
      "Episode 42 - Total score (averaged over agents): 0.11\n",
      "Episode 43 - Total score (averaged over agents): 0.13\n",
      "Episode 44 - Total score (averaged over agents): 0.05\n",
      "Episode 45 - Total score (averaged over agents): 0.17\n",
      "Episode 46 - Total score (averaged over agents): 0.06\n",
      "Episode 47 - Total score (averaged over agents): 0.09\n",
      "Episode 48 - Total score (averaged over agents): 0.09\n",
      "Episode 49 - Total score (averaged over agents): 0.11\n",
      "Episode 50 - Total score (averaged over agents): 0.14\n",
      "Episode 51 - Total score (averaged over agents): 0.12\n",
      "Episode 52 - Total score (averaged over agents): 0.10\n",
      "Episode 53 - Total score (averaged over agents): 0.06\n",
      "Episode 54 - Total score (averaged over agents): 0.20\n",
      "Episode 55 - Total score (averaged over agents): 0.15\n",
      "Episode 56 - Total score (averaged over agents): 0.15\n",
      "Episode 57 - Total score (averaged over agents): 0.21\n",
      "Episode 58 - Total score (averaged over agents): 0.17\n",
      "Episode 59 - Total score (averaged over agents): 0.16\n",
      "Episode 60 - Total score (averaged over agents): 0.13\n",
      "Episode 61 - Total score (averaged over agents): 0.05\n",
      "Episode 62 - Total score (averaged over agents): 0.06\n",
      "Episode 63 - Total score (averaged over agents): 0.10\n",
      "Episode 64 - Total score (averaged over agents): 0.07\n",
      "Episode 65 - Total score (averaged over agents): 0.09\n",
      "Episode 66 - Total score (averaged over agents): 0.10\n",
      "Episode 67 - Total score (averaged over agents): 0.13\n",
      "Episode 68 - Total score (averaged over agents): 0.13\n",
      "Episode 69 - Total score (averaged over agents): 0.06\n",
      "Episode 70 - Total score (averaged over agents): 0.02\n",
      "Episode 71 - Total score (averaged over agents): 0.15\n",
      "Episode 72 - Total score (averaged over agents): 0.05\n",
      "Episode 73 - Total score (averaged over agents): 0.11\n",
      "Episode 74 - Total score (averaged over agents): 0.12\n",
      "Episode 75 - Total score (averaged over agents): 0.13\n",
      "Episode 76 - Total score (averaged over agents): 0.06\n",
      "Episode 77 - Total score (averaged over agents): 0.07\n",
      "Episode 78 - Total score (averaged over agents): 0.10\n",
      "Episode 79 - Total score (averaged over agents): 0.23\n",
      "Episode 80 - Total score (averaged over agents): 0.08\n",
      "Episode 81 - Total score (averaged over agents): 0.08\n",
      "Episode 82 - Total score (averaged over agents): 0.08\n",
      "Episode 83 - Total score (averaged over agents): 0.16\n",
      "Episode 84 - Total score (averaged over agents): 0.22\n",
      "Episode 85 - Total score (averaged over agents): 0.14\n",
      "Episode 86 - Total score (averaged over agents): 0.19\n",
      "Episode 87 - Total score (averaged over agents): 0.17\n",
      "Episode 88 - Total score (averaged over agents): 0.15\n",
      "Episode 89 - Total score (averaged over agents): 0.09\n",
      "Episode 90 - Total score (averaged over agents): 0.06\n"
     ]
    }
   ],
   "source": [
    "episode = 1\n",
    "meanScores = []\n",
    "\n",
    "while True:\n",
    "    # let's restart the environment and get the initial states of the agents\n",
    "    env_info = env.reset( train_mode=True )[ brain_name ]    \n",
    "    states = env_info.vector_observations\n",
    "    episodeScores = np.zeros(agent_num)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # choose the actions we should take\n",
    "        actions = agent_overlord.choose_actions( states )\n",
    "\n",
    "        # apply them\n",
    "        env_info = env.step( actions )[ brain_name ]\n",
    "\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # update the overlord\n",
    "        agent_overlord.step( states, actions, rewards, dones )\n",
    "\n",
    "        # update states for next iteration\n",
    "        states = next_states\n",
    "\n",
    "        episodeScores += env_info.rewards\n",
    "\n",
    "        if np.any( dones ):\n",
    "            # episode is done\n",
    "            break\n",
    "\n",
    "    episodeMeanScore = np.mean(episodeScores)\n",
    "    meanScores.append( episodeMeanScore )\n",
    "    agent_overlord.clear()\n",
    "    \n",
    "    print('Episode {:d} - Total score (averaged over agents): {:.2f}'.format(episode, episodeMeanScore))\n",
    "    meanTotalScore = np.mean( meanScores )\n",
    "    if ( meanTotalScore >= 30 ):\n",
    "        print('Episode count was {}', format( episode ) )\n",
    "        print('Mean score was {}', meanTotalScore )\n",
    "        break\n",
    "    \n",
    "    episode = episode + 1\n",
    "    if episode > 300:\n",
    "        break\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(meanScores)), meanScores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
